{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visión por computadora II\n",
    "## Trabajo Práctico Integrador\n",
    "\n",
    "## Modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from time import time\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import dill as pickle\n",
    "\n",
    "from plotly import graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T, models\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el directorio de salida si no existe\n",
    "path_output = \"../output\"\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el directorio de logs si no existe para guardar los logs de tensorboard\n",
    "logs_dir = '../logs'\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos disponible GPU, lo usamos\n",
    "# Chequeamos si tenemos disponible GPU (CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# Chequeamos si tenemos disponible aceleración por hardware en un chip de Apple (MPS)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "# Por defecto usamos CPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semilla para reproducibilidad de los experimentos\n",
    "random.seed(72)\n",
    "np.random.seed(72)\n",
    "torch.manual_seed(72);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datasets preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data\"\n",
    "\n",
    "path_train = os.path.join(path, \"train\")\n",
    "path_test = os.path.join(path, \"test\")\n",
    "path_valid = os.path.join(path, \"valid\")\n",
    "print(\n",
    "    f\"train files: {len(os.listdir(path_train))}, \"\n",
    "    f\"test files: {len(os.listdir(path_test))}, \"\n",
    "    f\"valid files: {len(os.listdir(path_valid))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de train\n",
    "path_train_class = os.path.join(path, \"train_dataset_preprocesado.csv\")\n",
    "df_train = pd.read_csv(path_train_class)\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de test\n",
    "path_test_class = os.path.join(path, \"test_dataset_preprocesado.csv\")\n",
    "df_test = pd.read_csv(path_test_class)\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de valid\n",
    "path_valid_class = os.path.join(path, \"val_dataset_preprocesado.csv\")\n",
    "df_valid = pd.read_csv(path_valid_class)\n",
    "print(df_valid.shape)\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un rendimiento óptimo, resnet18 necesita una forma de entrada que sea múltiplo de 32 y en nuestro caso tenemos una entrada de tamaño 256. De 256, el múltiplo de 32 más cercano es 224.\n",
    "\n",
    "Los valores mean y std utilizados en las transformaciones de normalización (mean=[0.485, 0.456, 0.406] y std=[0.229, 0.224, 0.225]) son estadísticas pre-calculadas sobre el conjunto de datos ImageNet. ImageNet es un gran conjunto de datos de imágenes comúnmente utilizado para entrenar modelos de visión por computadora, incluyendo redes neuronales profundas como ResNet.\n",
    "\n",
    "Estos valores se utilizan para centrar los datos en torno a cero y escalar la varianza, lo cual puede ayudar a la red a entrenar más eficientemente. La idea es que al normalizar las imágenes con los mismos valores de mean y std con los que el modelo preentrenado fue entrenado, el rendimiento del modelo será mejor y más consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_transforms():\n",
    "    transform_train = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], # Media extraída de ImageNet\n",
    "            std=[0.229, 0.224, 0.225], # Desviación estándar extraída de ImageNet\n",
    "        )\n",
    "    ])\n",
    "    transform_val = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], # Media extraída de ImageNet\n",
    "            std=[0.229, 0.224, 0.225], # Desviación estándar extraída de ImageNet\n",
    "        )\n",
    "    ])\n",
    "    return transform_train, transform_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase de dataset personalizado para manipular los batchs de datos entre RAM y disco más fácilmente. \n",
    "\n",
    "Algunos puntos importantes:\n",
    "\n",
    "* `__init__`: Este es el constructor de la clase. Inicializa varias variables de instancia, incluyendo un DataFrame que contiene los datos, ohe_tags (etiquetas codificadas en one-hot), transform (una función de transformación para aplicar a las imágenes), path (la ruta o rutas a las imágenes), is_train (un booleano que indica si el conjunto de datos es para entrenamiento o prueba), y idx_tta (un índice para la técnica de aumento de test, [TTA](../referencias/TTA.md)). \n",
    "\n",
    "Es importante distinguir la fase de entrenamiento de la fase de prueba porque utilizamos el aumento de pruebas.\n",
    "El aumento de pruebas (TTA) es útil para diversificar nuestro conjunto de datos de entrenamiento y construir un modelo más sólido. Se aplica a cada imagen para cada lote, lo que significa que no aumenta la longitud de nuestro conjunto de datos de entrenamiento, pero transforma cada imagen aleatoriamente durante el tiempo de ejecución.\n",
    "\n",
    "* `__len__`: Este método devuelve la longitud del DataFrame, es decir, el número de elementos en el conjunto de datos.\n",
    "\n",
    "* `__getitem__`: Este método se utiliza para obtener un elemento del conjunto de datos dado un índice. Lee la imagen correspondiente del disco, la convierte de BGR a RGB, y devuelve la imagen y su etiqueta correspondiente.\n",
    "\n",
    "* `collate_fn`: Este método se utiliza para procesar un lote de imágenes y etiquetas. Aplica la función de transformación a cada imagen, las convierte en tensores, las permuta, y las apila en un tensor de mayor dimensión.\n",
    "\n",
    "* `load_img`: Este método carga una imagen y su etiqueta correspondiente del conjunto de datos y las muestra en una gráfica.\n",
    "\n",
    "* `custom_augment`: Este método aplica una serie de transformaciones a una imagen, incluyendo rotaciones y volteos. Las transformaciones son aleatorias durante el entrenamiento y no aleatorias durante las pruebas para la TTA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloWasteDatasetError(Exception):\n",
    "    pass\n",
    "\n",
    "class YoloWasteDataset(Dataset):\n",
    "    def __init__(self, df, ohe_tags, transform, path, is_train=True, idx_tta=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.ohe_tags = ohe_tags\n",
    "        self.transform = transform\n",
    "        if isinstance(path, str):\n",
    "            self.paths = [path]\n",
    "        elif isinstance(path, (list, tuple)):\n",
    "            self.paths = path\n",
    "        else:\n",
    "            raise YoloWasteDatasetError(f\"Path type must be str, list or tuple, got: {type(path)}\")\n",
    "        self.is_train = is_train\n",
    "        if not is_train:\n",
    "            if not idx_tta in list(range(6)):\n",
    "                raise YoloWasteDatasetError(\n",
    "                    f\"In test mode, 'idx_tta' must be an int belonging to [0, 5], got: {repr(idx_tta)}\"\n",
    "                )\n",
    "            self.idx_tta = idx_tta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.df.iloc[idx, 0] # Asumiendo que la primer columna es filename\n",
    "        for path in self.paths:\n",
    "            if filename in os.listdir(path):\n",
    "                file_path = os.path.join(path, filename)\n",
    "                break\n",
    "        else:\n",
    "            raise YoloWasteDatasetError(f\"Can't fetch {filename} among {self.paths}\")\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.ohe_tags[idx]\n",
    "        return img, label\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, labels = [], []\n",
    "        for (img, label) in batch:\n",
    "            img = self.custom_augment(img)\n",
    "            img = torch.tensor(img)\n",
    "            img = img.permute(2, 0, 1)\n",
    "            img = self.transform(img)\n",
    "            imgs.append(img[None])\n",
    "            labels.append(label)\n",
    "        imgs = torch.cat(imgs).float().to(device)\n",
    "        labels = torch.tensor(labels).float().to(device)\n",
    "        return imgs, labels\n",
    "\n",
    "    def load_img(self, idx, ax=None):\n",
    "        img, ohe_label = self[idx]\n",
    "        label = self.df.iloc[idx]\n",
    "        title = f\"{label} - {ohe_label}\"\n",
    "        if ax is None:\n",
    "            plt.imshow(img)\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(title)\n",
    "    \n",
    "    def custom_augment(self, img):\n",
    "        \"\"\"\n",
    "        Discrete rotation and horizontal flip.\n",
    "        Random during training and non random during testing for TTA.\n",
    "        Not implemented in torchvision.transforms, hence this function.\n",
    "        \"\"\"\n",
    "        choice = np.random.randint(0, 6) if self.is_train else self.idx_tta\n",
    "        if choice == 0:\n",
    "            # Rotate 90\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
    "        if choice == 1:\n",
    "            # Rotate 90 and flip horizontally\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
    "            img = cv2.flip(img, flipCode=1)\n",
    "        if choice == 2:\n",
    "            # Rotate 180\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n",
    "        if choice == 3:\n",
    "            # Rotate 180 and flip horizontally\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n",
    "            img = cv2.flip(img, flipCode=1)\n",
    "        if choice == 4:\n",
    "            # Rotate 90 counter-clockwise\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        if choice == 5:\n",
    "            # Rotate 90 counter-clockwise and flip horizontally\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            img = cv2.flip(img, flipCode=1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df_train, df_val, df_test, path_train, batch_size):\n",
    "    # Suponiendo que la primera columna es el nombre del archivo y el resto son las columnas de categoría\n",
    "    category_columns = df_train.columns[1:]\n",
    "\n",
    "    # Extraemos las etiquetas directamente del dataframe.\n",
    "    ohe_tags_train = df_train[category_columns].values\n",
    "    ohe_tags_val = df_val[category_columns].values\n",
    "    ohe_tags_test = df_test[category_columns].values\n",
    "\n",
    "    # Obtén las transformaciones\n",
    "    transform_train, transform_val = obtener_transforms()\n",
    "\n",
    "    # Crear datasets\n",
    "    ds_train = YoloWasteDataset(df_train, ohe_tags_train, transform_train, path=path_train)\n",
    "    ds_val = YoloWasteDataset(df_val, ohe_tags_val, transform_val, path=path_train)\n",
    "    ds_test = YoloWasteDataset(df_test, ohe_tags_test, transform_val, path=path_train)\n",
    "\n",
    "    # Crear dataloaders\n",
    "    dl_train = DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=ds_train.collate_fn\n",
    "    )\n",
    "    dl_val = DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=ds_val.collate_fn\n",
    "    )\n",
    "    dl_test = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=ds_test.collate_fn\n",
    "    )\n",
    "\n",
    "    return ds_train, ds_val, ds_test, dl_train, dl_val, dl_test\n",
    "\n",
    "# Ejemplo de uso\n",
    "# df_train = pd.read_csv('train.csv')\n",
    "# df_val = pd.read_csv('val.csv')\n",
    "# df_test = pd.read_csv('test.csv')\n",
    "# path_train = 'path/to/train/images'\n",
    "# ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_val, df_test, path_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_train, ds_val, dl_train, dl_val = get_data(df_train, df_valid, path_train)\n",
    "ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_valid, df_test, path_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(dl_train))\n",
    "imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.load_img(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos pesos directamente del resnet18 previamente entrenado y congelamos todos los pesos. Sobrescribimos la última capa completamente conectada agregando dos capas densas seguidas de un sigmoide. Esta última parte del fc es la única capa a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet18\n",
    "def get_resnet_model():\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights)\n",
    "    for param in model.parameters():\n",
    "        param.require_grad = False\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "    model.fc = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(512, 128),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(.2),\n",
    "      nn.Linear(128, 36),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descargamos EfficientNet-B0 previamente entrenado y congelamos todos los pesos. Descongelamos las ultimas capas. Y modificamos la capa final completamente conectada agregando dos capas densas seguidas de un sigmoide. Esta última parte del fc es la única capa a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet-B0\n",
    "def get_efficientnet_model():\n",
    "    weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = models.efficientnet_b0(weights=weights)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.features[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(model.classifier[1].in_features, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(128, 36),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descargamos Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformers ViT\n",
    "def get_vit_model():\n",
    "    #TODO: Implementar el modelo Vision Transformer (ViT)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descargamos Swin Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swin_transformer_model():\n",
    "    #TODO: Implementar el modelo Swin Transformer\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, Y, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    Y_hat = model(X)\n",
    "    batch_loss = loss_fn(Y_hat, Y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    Y_hat = Y_hat.detach().float().cpu().numpy()\n",
    "    \n",
    "    return Y_hat, batch_loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_val_loss(X, Y, model, loss_fn):\n",
    "    model.eval()\n",
    "    Y_hat = model(X)\n",
    "    batch_loss = loss_fn(Y_hat, Y)\n",
    "    Y_hat = Y_hat.detach().float().cpu().numpy()\n",
    "    \n",
    "    return Y_hat, batch_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos configurar por defecto nuestro modelo durante 15 épocas, mientras reducimos nuestra tasa de aprendizaje 10 veces cada 7 lotes. Monitoreamos que la pérdida de validación son nuestras métricas clave. La puntuación de validación es útil sólo como indicación secundaria, porque elegimos el umbral de clasificación de forma bastante aleatoria (0,2).\n",
    "Posteriormente encontraremos el umbral más adecuado para cada objetivo.\n",
    "\n",
    "Mas adelante efectuamos una optimización de hiperparámetros usando algoritmos genéticos. (Falta referencias)\n",
    "\n",
    "La métrica de evaluación será f1-beta. Revisamos varias: [Métricas](../referencias/Metricas_Evaluacion.md)\n",
    " - Es una generalización del F1-score que permite ajustar el equilibrio entre Precision y Recall mediante un parámetro beta \\( $\\beta$ \\). Un valor de \\( $\\beta$ > 1 \\) da más peso a Recall y \\( $\\beta$ < 1 \\) da más peso a Precision.\n",
    "\n",
    "- \\[ $\\text{F1-}\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}$ \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dl_train, dl_val, idx_fold, model, optimizer, loss_fn, model_name, epochs=15):\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f'{logs_dir}/{model_name}_fold{idx_fold}') # Para tensorboard\n",
    "    lr_scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # Reduce el learning rate por 10 cada 7 epochs\n",
    "\n",
    "    loss_train, loss_val = [], []\n",
    "    score_train, score_val = [], []\n",
    "\n",
    "    Y_hat_val = None\n",
    "    Y_thresh_val = None  # Inicializar con None para evitar errores\n",
    "    best_loss_val = np.inf\n",
    "\n",
    "    for idx in range(epochs):\n",
    "        loss_train_epoch, loss_val_epoch = [], []\n",
    "        Y_hat_train_epoch, Y_hat_val_epoch = [], []\n",
    "        Y_train_epoch, Y_val_epoch = [], []\n",
    "\n",
    "        for X, Y in tqdm(dl_train, leave=False):\n",
    "            Y_hat, batch_loss = train_batch(X, Y, model, loss_fn, optimizer)\n",
    "            loss_train_epoch.append(batch_loss)\n",
    "            Y_hat_train_epoch.extend(Y_hat)\n",
    "            Y_train_epoch.extend(Y.detach().float().cpu().numpy())\n",
    "\n",
    "        for X, Y in tqdm(dl_val, leave=False):\n",
    "            Y_hat, batch_loss = compute_val_loss(X, Y, model, loss_fn)\n",
    "            loss_val_epoch.append(batch_loss)\n",
    "            Y_hat_val_epoch.extend(Y_hat)\n",
    "            Y_val_epoch.extend(Y.detach().float().cpu().numpy())\n",
    "                \n",
    "        avg_loss_train = np.mean(loss_train_epoch)\n",
    "        avg_loss_val = np.mean(loss_val_epoch)\n",
    "\n",
    "        Y_hat_train_epoch = np.array(Y_hat_train_epoch)\n",
    "        Y_hat_val_epoch = np.array(Y_hat_val_epoch)\n",
    "        Y_thresh_train_epoch = (Y_hat_train_epoch > .2).astype(float)\n",
    "        Y_thresh_val_epoch = (Y_hat_val_epoch > .2).astype(float)\n",
    "        Y_train_epoch = np.array(Y_train_epoch)\n",
    "        Y_val_epoch = np.array(Y_val_epoch)\n",
    "        \n",
    "        score_train_epoch = fbeta_score(Y_train_epoch, Y_thresh_train_epoch, beta=2, average=\"samples\")\n",
    "        score_val_epoch = fbeta_score(Y_val_epoch, Y_thresh_val_epoch, beta=2, average=\"samples\")\n",
    "        \n",
    "               \n",
    "        # saving values for debugging\n",
    "        if avg_loss_val < best_loss_val:\n",
    "            best_loss_val = avg_loss_val\n",
    "            Y_hat_val = Y_hat_val_epoch\n",
    "            Y_thresh_val = Y_thresh_val_epoch\n",
    "            Y_val = Y_val_epoch\n",
    "            \n",
    "        loss_train.append(avg_loss_train)\n",
    "        loss_val.append(avg_loss_val)\n",
    "        score_train.append(score_train_epoch)\n",
    "        score_val.append(score_val_epoch)\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {idx}/{epochs} -- train loss: {avg_loss_train}, \" \\\n",
    "            f\"val loss: {avg_loss_val}\" \\\n",
    "            f\" -- train fbeta_score: {score_train_epoch}, \" \\\n",
    "            f\"val fbeta_score: {score_val_epoch}\"\n",
    "        )\n",
    "\n",
    "        # Guardar los valores en tensorboard\n",
    "        writer.add_scalar('Loss/train', avg_loss_train, idx)\n",
    "        writer.add_scalar('Loss/val', avg_loss_val, idx)\n",
    "        writer.add_scalar('Score/train', score_train_epoch, idx)\n",
    "        writer.add_scalar('Score/val', score_val_epoch, idx)\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "    writer.close() # Cerrar tensorboard\n",
    "\n",
    "    train_results = {\n",
    "        \"loss_train\": loss_train,\n",
    "        \"loss_val\": loss_val,\n",
    "        \"score_train\": score_train,\n",
    "        \"score_val\": score_val,\n",
    "        \"Y_hat_val\": Y_hat_val,\n",
    "        \"Y_thresh_val\": Y_thresh_val,\n",
    "        \"Y_val\": Y_val,\n",
    "    }\n",
    "        \n",
    "    \n",
    "    # Guardamos el modelo\n",
    "    torch.save(model, os.path.join(path_output, f\"{model_name}_fold{idx_fold}.pth\"))\n",
    "\n",
    "    # Guardar los resultados del entrenamiento\n",
    "    pickle.dump(train_results, open(f\"../output/train_results_{model_name}_fold{idx_fold}.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de mejores hiperparametros\n",
    "\n",
    "Realizamos la búsqueda de mejores hiperparámetros empleando Random Search.\n",
    "\n",
    "Intentamos previamente la búsqueda de mejores hiperparámetros empleando la librería DEAP (Distributed Evolutionary Algorithms in Python). Pero la mutación generaba valores anómalos del learning rate. Así que decidimos emplear Random Search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los límites de los hiperparámetros\n",
    "LIMITES = {\n",
    "    \"lr\": (1e-4, 2e-2),\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"num_epochs\": [5, 10, 15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de arquitecturas\n",
    "model_map = {\n",
    "    \"resnet18\": get_resnet_model,\n",
    "    \"efficientnet_b0\": get_efficientnet_model\n",
    "    #\"vit\": get_vit_model,\n",
    "    #\"swin_transformer\": get_swin_transformer_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para realizar Random Search\n",
    "def random_search(model_name, n_iter=10):\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        lr = random.uniform(*LIMITES[\"lr\"])\n",
    "        batch_size = random.choice(LIMITES[\"batch_size\"])\n",
    "        num_epochs = random.choice(LIMITES[\"num_epochs\"])\n",
    "\n",
    "        ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_valid, df_test, path_train, batch_size)\n",
    "\n",
    "        model_fn = model_map[model_name]\n",
    "        model = model_fn()\n",
    "        optimizer = Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        print(\"_____________________________________________________________________________________________\")\n",
    "        print(f\"Parámetros de optimización => lr: {lr}, batch_size: {batch_size}, num_epochs: {num_epochs}\")\n",
    "        print(\"_____________________________________________________________________________________________\")\n",
    "\n",
    "        train_model(dl_train, dl_val, 0, model, optimizer, loss_fn, model_name, epochs=num_epochs)\n",
    "\n",
    "        train_results = pickle.load(open(f\"../output/train_results_{model_name}_fold0.pkl\", \"rb\"))\n",
    "        score_val = max(train_results[\"score_val\"])\n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_params = (lr, batch_size, num_epochs)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(f\"Mejores parámetros encontrados => lr: {best_params[0]}, batch_size: {best_params[1]}, num_epochs: {best_params[2]}\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=../logs --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Al abrir en el navegador http://localhost:6006 veremos la evolución del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la optimización para cada modelo\n",
    "#model_names = [\"resnet18\", \"efficientnet_b0\", \"vit\", \"swin_transformer\"]\n",
    "model_names = [\"resnet18\", \"efficientnet_b0\"]\n",
    "for model_name in model_names:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Optimizando modelo: {model_name}\")\n",
    "    best_params = random_search(model_name, n_iter=15)\n",
    "        \n",
    "    lr, batch_size, num_epochs = best_params\n",
    "        \n",
    "    ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_valid, df_test, path_train, batch_size)\n",
    "\n",
    "    model_fn = model_map[model_name]\n",
    "    model = model_fn()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    print(\"\\n\") \n",
    "    print(\"===========================================================================================\")\n",
    "    print(f\"Entrenando el mejor modelo con lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "    print(\"===========================================================================================\")\n",
    "    train_model(dl_train, dl_val, 1, model, optimizer, loss_fn, model_name, epochs=num_epochs)\n",
    "\n",
    "    Y_hat_test, Y_test = [], []\n",
    "    for X, Y in tqdm(dl_test, leave=False):\n",
    "        Y_hat, _ = compute_val_loss(X, Y, model, loss_fn)\n",
    "        Y_hat_test.extend(Y_hat)\n",
    "        Y_test.extend(Y.detach().float().cpu().numpy())\n",
    "        \n",
    "    Y_hat_test = np.array(Y_hat_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_thresh_test = (Y_hat_test > .2).astype(float)\n",
    "        \n",
    "    final_score = fbeta_score(Y_test, Y_thresh_test, beta=2, average=\"samples\")\n",
    "    print(\"\\n\")\n",
    "    print(\"*************************************************************************************\")\n",
    "    print(f\"* Puntaje final de Fbeta en el conjunto de pruebas para {model_name}: {final_score}  *\")\n",
    "    print(\"**************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos los entrenamientos con el mejor modelo de cada arquitectura\n",
    "\n",
    "Si bien los entrenamientos pueden visualizarse empleando tensorboard, los graficaremos para presentarlos de otra manera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo Resnet18\n",
    "model = torch.load(os.path.join(path_output, \"resnet18_fold1.pth\"))\n",
    "\n",
    "# Cargamos los resultados del entrenamiento\n",
    "train_results = pickle.load(open(f\"../output/train_results_resnet18_fold1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = train_results[\"loss_train\"]\n",
    "loss_val = train_results[\"loss_val\"]\n",
    "score_train = train_results[\"score_train\"]\n",
    "score_val = train_results[\"score_val\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Fbeta scores\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_train))),\n",
    "        y=loss_train,\n",
    "        name=\"loss_train\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_val))),\n",
    "        y=loss_val,\n",
    "        name=\"loss_val\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_train))),\n",
    "        y=score_train,\n",
    "        name=\"score_train\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_val))),\n",
    "        y=score_val,\n",
    "        name=\"score_val\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo EfficientNet-B0\n",
    "model = torch.load(os.path.join(path_output, \"efficientnet_b0_fold1.pth\"))\n",
    "\n",
    "# Cargamos los resultados del entrenamiento\n",
    "train_results = pickle.load(open(f\"../output/train_results_efficientnet_b0_fold1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = train_results[\"loss_train\"]\n",
    "loss_val = train_results[\"loss_val\"]\n",
    "score_train = train_results[\"score_train\"]\n",
    "score_val = train_results[\"score_val\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Fbeta scores\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_train))),\n",
    "        y=loss_train,\n",
    "        name=\"loss_train\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_val))),\n",
    "        y=loss_val,\n",
    "        name=\"loss_val\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_train))),\n",
    "        y=score_train,\n",
    "        name=\"score_train\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_val))),\n",
    "        y=score_val,\n",
    "        name=\"score_val\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
