{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from time import time\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import dill as pickle\n",
    "\n",
    "from plotly import graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T, models\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "#!pip install -q torchsummary --user\n",
    "from torchsummary import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from deap import base, creator, tools, algorithms # Para el algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos disponible GPU, lo usamos\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(101)\n",
    "np.random.seed(101)\n",
    "torch.manual_seed(101);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datasets preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data\"\n",
    "path_output = \"../output\"\n",
    "\n",
    "path_train = os.path.join(path, \"train\")\n",
    "path_test = os.path.join(path, \"test\")\n",
    "path_valid = os.path.join(path, \"valid\")\n",
    "print(\n",
    "    f\"train files: {len(os.listdir(path_train))}, \"\n",
    "    f\"test files: {len(os.listdir(path_test))}, \"\n",
    "    f\"valid files: {len(os.listdir(path_valid))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de train\n",
    "path_train_class = os.path.join(path, \"train_dataset_preprocesado.csv\")\n",
    "df_train = pd.read_csv(path_train_class)\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de test\n",
    "path_test_class = os.path.join(path, \"test_dataset_preprocesado.csv\")\n",
    "df_test = pd.read_csv(path_test_class)\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de valid\n",
    "path_valid_class = os.path.join(path, \"val_dataset_preprocesado.csv\")\n",
    "df_valid = pd.read_csv(path_valid_class)\n",
    "print(df_valid.shape)\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un rendimiento óptimo, resnet18 necesita una forma de entrada que sea múltiplo de 32 y en nuestro caso tenemos una entrada de tamaño 256. De 256, el múltiplo de 32 más cercano es 224.\n",
    "\n",
    "Los valores mean y std utilizados en las transformaciones de normalización (mean=[0.485, 0.456, 0.406] y std=[0.229, 0.224, 0.225]) son estadísticas pre-calculadas sobre el conjunto de datos ImageNet. ImageNet es un gran conjunto de datos de imágenes comúnmente utilizado para entrenar modelos de visión por computadora, incluyendo redes neuronales profundas como ResNet.\n",
    "\n",
    "Estos valores se utilizan para centrar los datos en torno a cero y escalar la varianza, lo cual puede ayudar a la red a entrenar más eficientemente. La idea es que al normalizar las imágenes con los mismos valores de mean y std con los que el modelo preentrenado fue entrenado, el rendimiento del modelo será mejor y más consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_transforms():\n",
    "    transform_train = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "    ])\n",
    "    transform_val = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "    ])\n",
    "    return transform_train, transform_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase de dataset personalizado para manipular los bathcs de datos entre RAM y disco más fácilmente. \n",
    "\n",
    "Algunos puntos importantes:\n",
    "\n",
    "* `__init__`: Este es el constructor de la clase. Inicializa varias variables de instancia, incluyendo un DataFrame que contiene los datos, ohe_tags (etiquetas codificadas en one-hot), transform (una función de transformación para aplicar a las imágenes), path (la ruta o rutas a las imágenes), is_train (un booleano que indica si el conjunto de datos es para entrenamiento o prueba), y idx_tta (un índice para la técnica de aumento de test, [TTA](../referencias/TTA.md)). \n",
    "\n",
    "Es importante distinguir la fase de entrenamiento de la fase de prueba porque utilizamos el aumento de pruebas.\n",
    "El aumento de pruebas (TTA) es útil para diversificar nuestro conjunto de datos de entrenamiento y construir un modelo más sólido. Se aplica a cada imagen para cada lote, lo que significa que no aumenta la longitud de nuestro conjunto de datos de entrenamiento, pero transforma cada imagen aleatoriamente durante el tiempo de ejecución.\n",
    "\n",
    "* `__len__`: Este método devuelve la longitud del DataFrame, es decir, el número de elementos en el conjunto de datos.\n",
    "\n",
    "* `__getitem__`: Este método se utiliza para obtener un elemento del conjunto de datos dado un índice. Lee la imagen correspondiente del disco, la convierte de BGR a RGB, y devuelve la imagen y su etiqueta correspondiente.\n",
    "\n",
    "* `collate_fn`: Este método se utiliza para procesar un lote de imágenes y etiquetas. Aplica la función de transformación a cada imagen, las convierte en tensores, las permuta, y las apila en un tensor de mayor dimensión.\n",
    "\n",
    "* `load_img`: Este método carga una imagen y su etiqueta correspondiente del conjunto de datos y las muestra en una gráfica.\n",
    "\n",
    "* `custom_augment`: Este método aplica una serie de transformaciones a una imagen, incluyendo rotaciones y volteos. Las transformaciones son aleatorias durante el entrenamiento y no aleatorias durante las pruebas para la TTA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloWasteDatasetError(Exception):\n",
    "    pass\n",
    "\n",
    "class YoloWasteDataset(Dataset):\n",
    "    def __init__(self, df, ohe_tags, transform, path, is_train=True, idx_tta=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.ohe_tags = ohe_tags\n",
    "        self.transform = transform\n",
    "        if isinstance(path, str):\n",
    "            self.paths = [path]\n",
    "        elif isinstance(path, (list, tuple)):\n",
    "            self.paths = path\n",
    "        else:\n",
    "            raise YoloWasteDatasetError(f\"Path type must be str, list or tuple, got: {type(path)}\")\n",
    "        self.is_train = is_train\n",
    "        if not is_train:\n",
    "            if not idx_tta in list(range(6)):\n",
    "                raise YoloWasteDatasetError(\n",
    "                    f\"In test mode, 'idx_tta' must be an int belonging to [0, 5], got: {repr(idx_tta)}\"\n",
    "                )\n",
    "            self.idx_tta = idx_tta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #filename = self.df.iloc[idx, 0] + \".jpg\"  # Assuming the first column is the filename\n",
    "        filename = self.df.iloc[idx, 0] # Assuming the first column is the filename\n",
    "        for path in self.paths:\n",
    "            if filename in os.listdir(path):\n",
    "                file_path = os.path.join(path, filename)\n",
    "                break\n",
    "        else:\n",
    "            raise YoloWasteDatasetError(f\"Can't fetch {filename} among {self.paths}\")\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.ohe_tags[idx]\n",
    "        return img, label\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imgs, labels = [], []\n",
    "        for (img, label) in batch:\n",
    "            img = self.custom_augment(img)\n",
    "            img = torch.tensor(img)\n",
    "            img = img.permute(2, 0, 1)\n",
    "            img = self.transform(img)\n",
    "            imgs.append(img[None])\n",
    "            labels.append(label)\n",
    "        imgs = torch.cat(imgs).float().to(device)\n",
    "        labels = torch.tensor(labels).float().to(device)\n",
    "        return imgs, labels\n",
    "\n",
    "    def load_img(self, idx, ax=None):\n",
    "        img, ohe_label = self[idx]\n",
    "        #label = self.df.iloc[idx].tags\n",
    "        label = self.df.iloc[idx]\n",
    "        title = f\"{label} - {ohe_label}\"\n",
    "        if ax is None:\n",
    "            plt.imshow(img)\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(title)\n",
    "    \n",
    "    def custom_augment(self, img):\n",
    "        \"\"\"\n",
    "        Discrete rotation and horizontal flip.\n",
    "        Random during training and non random during testing for TTA.\n",
    "        Not implemented in torchvision.transforms, hence this function.\n",
    "        \"\"\"\n",
    "        choice = np.random.randint(0, 6) if self.is_train else self.idx_tta\n",
    "        if choice == 0:\n",
    "            # Rotate 90\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
    "        if choice == 1:\n",
    "            # Rotate 90 and flip horizontally\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
    "            img = cv2.flip(img, flipCode=1)\n",
    "        if choice == 2:\n",
    "            # Rotate 180\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n",
    "        if choice == 3:\n",
    "            # Rotate 180 and flip horizontally\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n",
    "            img = cv2.flip(img, flipCode=1)\n",
    "        if choice == 4:\n",
    "            # Rotate 90 counter-clockwise\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        if choice == 5:\n",
    "            # Rotate 90 counter-clockwise and flip horizontally\n",
    "            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            img = cv2.flip(img, flipCode=1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df_train, df_val, df_test, path_train, batch_size):\n",
    "    # Suponiendo que la primera columna es el nombre del archivo y el resto son las columnas de categoría\n",
    "    category_columns = df_train.columns[1:]\n",
    "\n",
    "    # Extraemos las etiquetas directamente del dataframe.\n",
    "    ohe_tags_train = df_train[category_columns].values\n",
    "    ohe_tags_val = df_val[category_columns].values\n",
    "    ohe_tags_test = df_test[category_columns].values\n",
    "\n",
    "    # Obtén las transformaciones\n",
    "    transform_train, transform_val = obtener_transforms()\n",
    "\n",
    "    # Crear datasets\n",
    "    ds_train = YoloWasteDataset(df_train, ohe_tags_train, transform_train, path=path_train)\n",
    "    ds_val = YoloWasteDataset(df_val, ohe_tags_val, transform_val, path=path_train)\n",
    "    ds_test = YoloWasteDataset(df_test, ohe_tags_test, transform_val, path=path_train)\n",
    "\n",
    "    # Crear dataloaders\n",
    "    dl_train = DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=ds_train.collate_fn\n",
    "    )\n",
    "    dl_val = DataLoader(\n",
    "        ds_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=ds_val.collate_fn\n",
    "    )\n",
    "    dl_test = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=ds_test.collate_fn\n",
    "    )\n",
    "\n",
    "    return ds_train, ds_val, ds_test, dl_train, dl_val, dl_test\n",
    "\n",
    "# Ejemplo de uso\n",
    "# df_train = pd.read_csv('train.csv')\n",
    "# df_val = pd.read_csv('val.csv')\n",
    "# df_test = pd.read_csv('test.csv')\n",
    "# path_train = 'path/to/train/images'\n",
    "# ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_val, df_test, path_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_train, ds_val, dl_train, dl_val = get_data(df_train, df_valid, path_train)\n",
    "ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_valid, df_test, path_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(dl_train))\n",
    "imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.load_img(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos pesos directamente del resnet18 previamente entrenado y congelamos todos los pesos. Sobrescribimos la última capa completamente conectada agregando dos capas densas seguidas de un sigmoide. Esta última parte del fc es la única capa a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet18\n",
    "def get_resnet_model():\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights)\n",
    "    for param in model.parameters():\n",
    "        param.require_grad = False\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "    model.fc = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(512, 128), # 512 for resnet18 or 2048 for resnet 50\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(.2),\n",
    "      nn.Linear(128, 36),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    return model.to(device), optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descargamos EfficientNet-B0 previamente entrenado y congelamos todos los pesos. Descongelamos las ultimas capas. Y modificamos la capa final completamente conectada agregando dos capas densas seguidas de un sigmoide. Esta última parte del fc es la única capa a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet-B0\n",
    "def get_efficientnet_model():\n",
    "    # Cargar el modelo EfficientNet-B0 preentrenado\n",
    "    weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = models.efficientnet_b0(weights=weights)\n",
    "    \n",
    "    # Congelar los parámetros del modelo\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Descongelar las últimas capas\n",
    "    for param in model.features[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Modificar la capa final para que se adapte a nuestra tarea específica\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(model.classifier[1].in_features, 128), # Obtener el tamaño de entrada de la capa final\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(128, 36),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=1e-2)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    return model.to(device), optimizer, loss_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descargamos Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformers ViT\n",
    "def get_vit_model():\n",
    "    #TODO: Implementar el modelo Vision Transformer (ViT)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descargamos Swin Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swin_transformer_model():\n",
    "    #TODO: Implementar el modelo Swin Transformer\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, Y, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    Y_hat = model(X)\n",
    "    batch_loss = loss_fn(Y_hat, Y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    Y_hat = Y_hat.detach().float().cpu().numpy()\n",
    "    \n",
    "    return Y_hat, batch_loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_val_loss(X, Y, model, loss_fn):\n",
    "    model.eval()\n",
    "    Y_hat = model(X)\n",
    "    batch_loss = loss_fn(Y_hat, Y)\n",
    "    Y_hat = Y_hat.detach().float().cpu().numpy()\n",
    "    \n",
    "    return Y_hat, batch_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos entrenar nuestro modelo durante 15 épocas por defecto, mientras reducimos nuestra tasa de aprendizaje 10 veces cada 7 lotes. Monitoreamos que la pérdida de validación son nuestras métricas clave. La puntuación de validación es útil sólo como indicación secundaria, porque elegimos el umbral de clasificación de forma bastante aleatoria (0,2).\n",
    "Posteriormente encontraremos el umbral más adecuado para cada objetivo.\n",
    "\n",
    "Mas adelante efectuamos una optimización de hiperparámetros usando algoritmos genéticos. (Falta referencias)\n",
    "\n",
    "La métrica de evaluación será f1-beta. Revisamos varias: [Métricas](../referencias/Metricas_Evaluacion.md)\n",
    " - Es una generalización del F1-score que permite ajustar el equilibrio entre Precision y Recall mediante un parámetro beta \\( $\\beta$ \\). Un valor de \\( $\\beta$ > 1 \\) da más peso a Recall y \\( $\\beta$ < 1 \\) da más peso a Precision.\n",
    "\n",
    "- \\[ $\\text{F1-}\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}$ \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dl_train, dl_val, idx_fold, model, optimizer, loss_fn, model_name, epochs=15):\n",
    "    lr_scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # Reduce el learning rate por 10 cada 7 epochs\n",
    "\n",
    "    loss_train, loss_val = [], []\n",
    "    score_train, score_val = [], []\n",
    "\n",
    "    Y_hat_val = None\n",
    "    best_loss_val = np.inf\n",
    "\n",
    "    for idx in range(epochs):\n",
    "        loss_train_epoch, loss_val_epoch = [], []\n",
    "        Y_hat_train_epoch, Y_hat_val_epoch = [], []\n",
    "        Y_train_epoch, Y_val_epoch = [], []\n",
    "\n",
    "        for X, Y in tqdm(dl_train, leave=False):\n",
    "            Y_hat, batch_loss = train_batch(X, Y, model, loss_fn, optimizer)\n",
    "            loss_train_epoch.append(batch_loss)\n",
    "            Y_hat_train_epoch.extend(Y_hat)\n",
    "            Y_train_epoch.extend(Y.detach().float().cpu().numpy())\n",
    "\n",
    "        for X, Y in tqdm(dl_val, leave=False):\n",
    "            Y_hat, batch_loss = compute_val_loss(X, Y, model, loss_fn)\n",
    "            loss_val_epoch.append(batch_loss)\n",
    "            Y_hat_val_epoch.extend(Y_hat)\n",
    "            Y_val_epoch.extend(Y.detach().float().cpu().numpy())\n",
    "                \n",
    "        avg_loss_train = np.mean(loss_train_epoch)\n",
    "        avg_loss_val = np.mean(loss_val_epoch)\n",
    "\n",
    "        Y_hat_train_epoch = np.array(Y_hat_train_epoch)\n",
    "        Y_hat_val_epoch = np.array(Y_hat_val_epoch)\n",
    "        Y_thresh_train_epoch = (Y_hat_train_epoch > .2).astype(float)\n",
    "        Y_thresh_val_epoch = (Y_hat_val_epoch > .2).astype(float)\n",
    "        Y_train_epoch = np.array(Y_train_epoch)\n",
    "        Y_val_epoch = np.array(Y_val_epoch)\n",
    "        \n",
    "        score_train_epoch = fbeta_score(Y_train_epoch, Y_thresh_train_epoch, beta=2, average=\"samples\")\n",
    "        score_val_epoch = fbeta_score(Y_val_epoch, Y_thresh_val_epoch, beta=2, average=\"samples\")\n",
    "               \n",
    "        # saving values for debugging\n",
    "        if avg_loss_val < best_loss_val:\n",
    "            best_loss_val = avg_loss_val\n",
    "            Y_hat_val = Y_hat_val_epoch\n",
    "            Y_thresh_val = Y_thresh_val_epoch\n",
    "            Y_val = Y_val_epoch\n",
    "            \n",
    "        loss_train.append(avg_loss_train)\n",
    "        loss_val.append(avg_loss_val)\n",
    "        score_train.append(score_train_epoch)\n",
    "        score_val.append(score_val_epoch)\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {idx}/{epochs} -- train loss: {avg_loss_train}, \" \\\n",
    "            f\"val loss: {avg_loss_val}\" \\\n",
    "            f\" -- train fbeta_score: {score_train_epoch}, \" \\\n",
    "            f\"val fbeta_score: {score_val_epoch}\"\n",
    "        )\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "\n",
    "    train_results = {\n",
    "        \"loss_train\": loss_train,\n",
    "        \"loss_val\": loss_val,\n",
    "        \"score_train\": score_train,\n",
    "        \"score_val\": score_val,\n",
    "        \"Y_hat_val\": Y_hat_val,\n",
    "        \"Y_thresh_val\": Y_thresh_val,\n",
    "        \"Y_val\": Y_val,\n",
    "    }\n",
    "        \n",
    "    \n",
    "    # Guardamos el modelo\n",
    "    torch.save(model, os.path.join(path_output, f\"{model_name}_fold{idx_fold}.pth\"))\n",
    "\n",
    "    # Guardar los resultados del entrenamiento\n",
    "    pickle.dump(train_results, open(f\"../output/train_results_{model_name}_fold{idx_fold}.pkl\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de mejores hiperparametros\n",
    "\n",
    "Realizamos la búsqueda de mejores hiperparámetros empleando la librería DEAP (Distributed Evolutionary Algorithms in Python).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el problema de optimización\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los límites de los hiperparámetros\n",
    "LIMITES = {\n",
    "    \"lr\": (1e-5, 1e-2),\n",
    "    \"batch_size\": (16, 128),\n",
    "    \"num_epochs\": (10, 30),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una función para inicializar los individuos\n",
    "def init_individuo(icls, bounds):\n",
    "    genome = []\n",
    "    for key in bounds:\n",
    "        if key == \"batch_size\" or key == \"num_epochs\":\n",
    "            genome.append(random.randint(bounds[key][0], bounds[key][1]))\n",
    "        else:\n",
    "            genome.append(random.uniform(bounds[key][0], bounds[key][1]))\n",
    "    return icls(genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de arquitecturas\n",
    "model_map = {\n",
    "    \"resnet18\": get_resnet_model,\n",
    "    \"efficientnet_b0\": get_efficientnet_model,\n",
    "    \"vit\": get_vit_model,\n",
    "    \"swin_transformer\": get_swin_transformer_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar los individuos\n",
    "def evaluate(individual, model_name):\n",
    "    lr, batch_size, num_epochs = individual\n",
    "\n",
    "    # Asegurar que batch_size y num_epochs sean enteros\n",
    "    batch_size = int(batch_size)\n",
    "    num_epochs = int(num_epochs)\n",
    "    \n",
    "    # Preparamos los datos\n",
    "    ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_valid, df_test, path_train, batch_size)\n",
    "    \n",
    "    # Inicializamos el modelo y los hiperparámetros\n",
    "    model_fn = model_map[model_name]\n",
    "    model, optimizer, loss_fn = model_fn()\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    # Entrenamos el modelo\n",
    "    train_model(dl_train, dl_val, 0, model, optimizer, loss_fn, model_name, epochs=num_epochs)\n",
    "    \n",
    "    # Cargamos los resultados del entrenamiento\n",
    "    train_results = pickle.load(open(f\"../output/train_results_{model_name}_fold0.pkl\", \"rb\"))\n",
    "    score_val = train_results[\"score_val\"]\n",
    "    \n",
    "    # Devolvemos el mejor score de validación (fbeta_score)\n",
    "    return (max(score_val),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", init_individuo, creator.Individual, LIMITES)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate, model_name=\"resnet18\")  # Default model\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=[b[0] for b in LIMITES.values()], up=[b[1] for b in LIMITES.values()], eta=1.0, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta el algoritmo genético\n",
    "def genetico(model_name):\n",
    "    toolbox.unregister(\"evaluate\")\n",
    "    toolbox.register(\"evaluate\", evaluate, model_name=model_name)\n",
    "    \n",
    "    population = toolbox.population(n=10)\n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    \n",
    "    algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, stats=stats, halloffame=hall_of_fame, verbose=True)\n",
    "    \n",
    "    return population, stats, hall_of_fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la optimización para cada modelo\n",
    "#model_names = [\"resnet18\"]\n",
    "model_names = [\"resnet18\", \"efficientnet_b0\", \"vit\", \"swin_transformer\"]\n",
    "for model_name in model_names:\n",
    "    print(f\"Optimizando modelo: {model_name}\")\n",
    "    population, stats, hall_of_fame = genetico(model_name)\n",
    "        \n",
    "    # Entrenamos el mejor modelo encontrado usando el conjunto de entrenamiento completo y validación\n",
    "    best_individual = hall_of_fame[0]\n",
    "    lr, batch_size, num_epochs = best_individual\n",
    "    \n",
    "    # Aseguramos que los valores sean del tipo correcto\n",
    "    lr = float(lr)\n",
    "    batch_size = int(batch_size)\n",
    "    num_epochs = int(num_epochs)\n",
    "        \n",
    "    ds_train, ds_val, ds_test, dl_train, dl_val, dl_test = get_data(df_train, df_valid, df_test, path_train, batch_size)\n",
    "    model_fn = model_map[model_name]\n",
    "    model, optimizer, loss_fn = model_fn()\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "    # Aca le colocamos al fold 1, para que no se guarde en el mismo archivo. \n",
    "    print(f\"Entrenando el mejor modelo con lr={lr}, batch_size={batch_size}, num_epochs={num_epochs}\")\n",
    "    train_model(dl_train, dl_val, 1, model, optimizer, loss_fn, model_name, epochs=num_epochs)\n",
    "        \n",
    "    # Evaluación final en el conjunto de datos de test\n",
    "    Y_hat_test, Y_test = [], []\n",
    "    for X, Y in tqdm(dl_test, leave=False):\n",
    "        Y_hat, _ = compute_val_loss(X, Y, model, loss_fn)\n",
    "        Y_hat_test.extend(Y_hat)\n",
    "        Y_test.extend(Y.detach().float().cpu().numpy())\n",
    "        \n",
    "    Y_hat_test = np.array(Y_hat_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    Y_thresh_test = (Y_hat_test > .2).astype(float)\n",
    "        \n",
    "    final_score = fbeta_score(Y_test, Y_thresh_test, beta=2, average=\"samples\")\n",
    "    print(f\"Puntaje final de Fbeta en el conjunto de pruebas para {model_name}: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joss y Natanael, Faltaria:\n",
    "\n",
    "- los gráficos, Natanael dijo que podrian ser con Tensorboard. Revisar en el código donde se efectua la optimización de hiperparámetrso, ahi se guaradn los archivos que luego pueden usarse para graficar. Tanto en la búsqueda del mejor, y una vez encontrado entrenar con ese mejor. Tambien se guarda. Para el primero se guarda con un 0 en el nombre de los archivos. Y para el entrenamiento con lo mejor con el 1 en el nombre del archivo.\n",
    "Fijense que enrena con el mejor (guarda el modelo entrenado y los scores de cada epoca) y al final hace el testeo con el set de test. Eso habria que graficarlo igual.\n",
    "\n",
    "- Referencias a la técnica de usar algoritmos genéticos para optimización de hiperparámetros\n",
    "\n",
    "- Primero dije que podriamos emplear 4 modelos. Pero se hace pesado buscar los mejores hiperparámetros de los 4 y luego entrenerlos. Habria que dejar todo ok y dejar ejecutando todo un dia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos los entrenamientos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo Resnet18\n",
    "model = torch.load(os.path.join(path_output, \"resnet18_fold1.pth\"))\n",
    "\n",
    "# Cargamos los resultados del entrenamiento\n",
    "train_results = pickle.load(open(f\"../output/train_results_resnet18_fold1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = train_results[\"loss_train\"]\n",
    "loss_val = train_results[\"loss_val\"]\n",
    "score_train = train_results[\"score_train\"]\n",
    "score_val = train_results[\"score_val\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Fbeta scores\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_train))),\n",
    "        y=loss_train,\n",
    "        name=\"loss_train\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_val))),\n",
    "        y=loss_val,\n",
    "        name=\"loss_val\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_train))),\n",
    "        y=score_train,\n",
    "        name=\"score_train\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_val))),\n",
    "        y=score_val,\n",
    "        name=\"score_val\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo EfficientNet-B0\n",
    "model = torch.load(os.path.join(path_output, \"EfficientNet-B0_fold1.pth\"))\n",
    "\n",
    "# Cargamos los resultados del entrenamiento\n",
    "train_results = pickle.load(open(f\"../output/train_results_EfficientNet-B0_fold1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = train_results[\"loss_train\"]\n",
    "loss_val = train_results[\"loss_val\"]\n",
    "score_train = train_results[\"score_train\"]\n",
    "score_val = train_results[\"score_val\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Fbeta scores\"))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_train))),\n",
    "        y=loss_train,\n",
    "        name=\"loss_train\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(loss_val))),\n",
    "        y=loss_val,\n",
    "        name=\"loss_val\",\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_train))),\n",
    "        y=score_train,\n",
    "        name=\"score_train\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(score_val))),\n",
    "        y=score_val,\n",
    "        name=\"score_val\",\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
